{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc690c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from judgemark_v2lp.utils.file_io import load_json_file, save_json_file\n",
    "from judgemark_v2lp.benchmark import sanitize_model_name, finalize_scores_and_compute_judgemark\n",
    "import uuid\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id=None\n",
    "\n",
    "samples_file = \"../data/judgemark_v2.1_samples.json\"\n",
    "samples_data = load_json_file(samples_file)\n",
    "json_file = \"outputs/my_judgemark_runs.json\"\n",
    "runs = load_json_file(json_file)\n",
    "runs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ad718",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0834fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_jm = sanitize_model_name(judge_model)\n",
    "base_id = run_id if run_id else str(uuid.uuid4())\n",
    "run_key = f\"{base_id}__{sanitized_jm}\"\n",
    "# Compute final stats\n",
    "finalize_scores_and_compute_judgemark(runs, run_key, samples_data, score_key=\"aggregated_score_raw\")\n",
    "finalize_scores_and_compute_judgemark(runs, run_key, samples_data, score_key=\"aggregated_score_weighted\")\n",
    "finalize_scores_and_compute_judgemark(runs, run_key, samples_data, score_key=\"aggregated_score_ranked\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
